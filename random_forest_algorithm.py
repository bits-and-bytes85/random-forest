# -*- coding: utf-8 -*-
"""Random Forest Algorithm

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12iRGAjENHhLT8hQDIGrWv2VBeAwyj6r3
"""

import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
from collections import defaultdict

class node:
    def __init__(self, attribute=None, threshold=None, label=None):
        self.attribute = attribute
        self.threshold = threshold
        self.label = label
        self.children = {}
        self.left = None            # for numerical attributes: value <= threshold
        self.right = None           # for numerical attributes: value > threshold

class decision_tree:
    def __init__(self, D, L, labels, attribute_types=None):
        """
        D: dataset
        L: list of attributes
        labels: class labels
        attribute_types: dictionary mapping attribute names to types ('categorical' or 'numerical')
        """

        self.attribute_types = attribute_types
        self.tree = self._build_tree(D, list(L), labels)

    def _gini_impurity(self, labels):
        """Calculate the Gini impurity of a set of labels."""
        if not labels:
            return 0
        counts = {}
        for label in labels:
            if label not in counts:
                counts[label] = 0
            counts[label] += 1

        impurity = 1.0
        total = len(labels)
        for count in counts.values():
            probability = count / total
            impurity -= probability ** 2
        return impurity

    def _numerical_split_gini(self, D, labels, attribute):
        """Find the best threshold for a numerical attribute using Gini index."""
        values = [float(instance[attribute]) for instance in D]

        if len(set(values)) <= 1:
            return None, float('inf')

        threshold = sum(values) / len(values)

        # gini impurity
        left_indices = [i for i, val in enumerate(values) if val <= threshold]
        right_indices = [i for i, val in enumerate(values) if val > threshold]

        left_labels = [labels[i] for i in left_indices]
        right_labels = [labels[i] for i in right_indices]


        if not left_labels or not right_labels:
            return threshold, float('inf')

        # weighted Gini impurity
        total = len(labels)
        left_weight = len(left_labels) / total
        right_weight = len(right_labels) / total

        left_impurity = self._gini_impurity(left_labels)
        right_impurity = self._gini_impurity(right_labels)

        weighted_impurity = left_weight * left_impurity + right_weight * right_impurity

        return threshold, weighted_impurity

    def _categorical_split_gini(self, D, labels, attribute):
        """Calculate the Gini impurity for a categorical split."""
        impurity_before = self._gini_impurity(labels)

        attribute_values = set(instance[attribute] for instance in D)

        weighted_impurity = 0
        total_instances = len(D)

        for value in attribute_values:
            subset_indices = [i for i, instance in enumerate(D) if instance[attribute] == value]
            subset_labels = [labels[i] for i in subset_indices]

            weight = len(subset_indices) / total_instances
            weighted_impurity += weight * self._gini_impurity(subset_labels)

        return weighted_impurity

    def _choose_best_attribute(self, D, L, labels, random_subset=False):
        if not L:
            return None, None

        if random_subset:
          m = int(len(L) ** 0.5)
          L = random.sample(L, max(1, m))

        best_attribute = None
        best_gain = -float('inf')
        best_threshold = None

        current_impurity = self._gini_impurity(labels)

        for attribute in L:
            if self.attribute_types[attribute] == 'categorical':
                impurity_after = self._categorical_split_gini(D, labels, attribute)
                gain = current_impurity - impurity_after
            else:  # numerical
                threshold, impurity_after = self._numerical_split_gini(D, labels, attribute)
                gain = current_impurity - impurity_after

                if threshold is None:
                    continue

            if gain > best_gain:
                best_gain = gain
                best_attribute = attribute
                best_threshold = threshold if self.attribute_types[attribute] == 'numerical' else None

        return best_attribute, best_threshold

    def _build_tree(self, D, L, labels):
        #ll examples have the same class
        if len(set(labels)) == 1:
            return node(label=labels[0])

        if not L:
            majority_class = max(set(labels), key=list(labels).count)
            return node(label=majority_class)


        A, threshold = self._choose_best_attribute(D, L, labels)

        if A is None:
            majority_class = max(set(labels), key=list(labels).count)
            return node(label=majority_class)

        # new decision tree node
        root = node(attribute=A, threshold=threshold)
        L_remaining = [attr for attr in L if attr != A]

        if self.attribute_types[A] == 'categorical':
            unique_values = set(instance[A] for instance in D)

            for value in unique_values:
                Dv_indices = [i for i, instance in enumerate(D) if instance[A] == value]
                Dv = [D[i] for i in Dv_indices]
                Dv_labels = [labels[i] for i in Dv_indices]

                if not Dv:
                    majority_class = max(set(labels), key=list(labels).count)
                    child_node = node(label=majority_class)
                else:
                    child_node = self._build_tree(Dv, L_remaining, Dv_labels)

                root.children[value] = child_node

        else:
            #split based on threshold
            left_indices = [i for i, instance in enumerate(D) if float(instance[A]) <= threshold]
            right_indices = [i for i, instance in enumerate(D) if float(instance[A]) > threshold]

            left_D = [D[i] for i in left_indices]
            left_labels = [labels[i] for i in left_indices]

            right_D = [D[i] for i in right_indices]
            right_labels = [labels[i] for i in right_indices]

            if not left_D:
                majority_class = max(set(labels), key=list(labels).count)
                root.left = node(label=majority_class)
            else:
                root.left = self._build_tree(left_D, L, left_labels)

            if not right_D:
                majority_class = max(set(labels), key=list(labels).count)
                root.right = node(label=majority_class)
            else:
                root.right = self._build_tree(right_D, L, right_labels)

        return root

    def predict(self, instance):
        current = self.tree

        while current.label is None:
            attribute = current.attribute

            if self.attribute_types[attribute] == 'categorical':
                value = instance.get(attribute)

                if value not in current.children:
                    labels = []
                    def collect_labels(node):
                        if node.label is not None:
                            labels.append(node.label)
                        if hasattr(node, 'children') and node.children:
                            for child in node.children.values():
                                collect_labels(child)
                        if hasattr(node, 'left') and node.left:
                            collect_labels(node.left)
                        if hasattr(node, 'right') and node.right:
                            collect_labels(node.right)
                    collect_labels(current)

                    if labels:
                        return max(set(labels), key=labels.count)
                    else:
                        return None

                current = current.children[value]
            else:
                try:
                    value = float(instance.get(attribute, 0))
                    if value <= current.threshold:
                        current = current.left
                    else:
                        current = current.right
                except (ValueError, TypeError):
                    labels = []
                    def collect_labels(node):
                        if node.label is not None:
                            labels.append(node.label)
                        if hasattr(node, 'left') and node.left:
                            collect_labels(node.left)
                        if hasattr(node, 'right') and node.right:
                            collect_labels(node.right)
                    collect_labels(current)

                    if labels:
                        return max(set(labels), key=labels.count)
                    else:
                        return None

        return current.label

class RandomForest:
    def __init__(self, ntree=10, attribute_types=None):
        self.ntree = ntree
        self.trees = []

    def get_attribute_types(self, D):
      return {
          attr: 'categorical' if isinstance(D[0][attr], str) else 'numerical'
          for attr in D[0]
    }

    def fit(self, D, L, labels):
        self.attribute_types = self.get_attribute_types(D)
        self.trees = []
        for _ in range(self.ntree):
            sample_D, sample_labels = self.bootstrap_sample(D, labels)
            tree = decision_tree(sample_D, list(L), sample_labels, self.attribute_types)
            tree._choose_best_attribute = lambda D, L, labels: tree._choose_best_attribute(D, L, labels, random_subset=True)
            self.trees.append(tree)

    def bootstrap_sample(self, D, labels):
        n = len(D)
        indices = [random.randint(0, n - 1) for _ in range(n)]
        sample_D = [D[i] for i in indices]
        sample_labels = [labels[i] for i in indices]
        return sample_D, sample_labels
    def predict(self, instance):
        votes = [tree.predict(instance) for tree in self.trees]
        return max(set(votes), key=votes.count)

def stratified_k_fold_split(D, labels, k=5, seed=42):
    random.seed(seed)
    label_to_indices = defaultdict(list)

    for i, label in enumerate(labels):
        label_to_indices[label].append(i)

    folds = [[] for _ in range(k)]

    for indices in label_to_indices.values():
        random.shuffle(indices)
        for i, idx in enumerate(indices):
            folds[i % k].append(idx)

    return folds

def get_metrics(true, pred):
    TP = FP = TN = FN = 0
    for t, p in zip(true, pred):
            if t == 1 and p == 1:
                TP += 1
            elif t == 0 and p == 1:
                FP += 1
            elif t == 1 and p == 0:
                FN += 1
            else:
                TN += 1
    precision = TP / (TP + FP)
    recall = TP / (TP + FN)
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    return accuracy, precision, recall, f1

loan_data = pd.read_csv('loan.csv')
cancer_data = pd.read_csv('wdbc.csv')
raisin_data = pd.read_csv('raisin.csv')
titanic_data = pd.read_csv('titanic.csv')

def evaluate_nn_stratified(data, title):
    labels = data['label']
    features = data.drop('label', axis=1)
    attribute_names = features.columns

    k=5
    folds = stratified_k_fold_split(features, labels, k=k)

    n_vs_accuracy = []
    n_vs_recall = []
    n_vs_precision = []
    n_vs_f1 = []

    n_tree_vals = [1, 5, 10, 20, 30, 40, 50]

    for j in n_tree_vals:
      avg_acc = 0
      avg_recall = 0
      avg_precision = 0
      avg_f1 = 0
      for i in range(5):
            test_indices = folds[i]
            train_indices = [idx for fold in folds[:i] + folds[i+1:] for idx in fold]

            train_D = features.iloc[train_indices].to_dict('records')
            train_labels = [labels[j] for j in train_indices]
            test_D = features.iloc[test_indices].to_dict('records')
            test_labels = [labels[j] for j in test_indices]

            rf = RandomForest(ntree=j)
            rf.fit(train_D, attribute_names, train_labels)

            preds = [rf.predict(x) for x in test_D]
            acc, precision, recall, f1 = get_metrics(test_labels, preds)
            avg_acc += acc
            avg_recall += recall
            avg_precision += precision
            avg_f1 += f1
      n_vs_accuracy.append(avg_acc/5)
      n_vs_recall.append(avg_recall/5)
      n_vs_precision.append(avg_precision/5)
      n_vs_f1.append(avg_f1/5)
    # final_accuracy = sum(accuracies) / len(accuracies)
    # print(f"\nAverage Accuracy over {k} folds: {final_accuracy:.4f}")
    print(n_vs_accuracy)
    print(n_vs_recall)
    print(n_vs_precision)
    print(n_vs_f1)

    plt.figure(figsize=(10, 6))
    plt.plot(n_tree_vals, n_vs_accuracy, label='N vs. Accuracy', marker='s')
    plt.xlabel('Number of trees')

    plt.ylabel('Accuracy')
    plt.title(title + ' Accuracy with Varying N')
    plt.legend()
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(10, 6))
    plt.plot(n_tree_vals, n_vs_recall, label='N vs. Recall', marker='o')
    plt.xlabel('Number of trees')

    plt.ylabel('Recall')
    plt.title(title + ' Recall with Varying N')
    plt.legend()
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(10, 6))
    plt.plot(n_tree_vals, n_vs_precision, label='N vs. Precision', marker='s')
    plt.xlabel('Number of trees')

    plt.ylabel('Precision')
    plt.title(title + ' Precision with Varying N')
    plt.legend()
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(10, 6))
    plt.plot(n_tree_vals, n_vs_f1, label='N vs. F1 Score', marker='o')
    plt.xlabel('Number of trees')

    plt.ylabel('F1')
    plt.title(title + 'F1 Score with Varying N')
    plt.legend()
    plt.grid(True)
    plt.show()

evaluate_random_forest_stratified(loan_data, "Loan")

evaluate_random_forest_stratified(cancer_data, "Cancer")

evaluate_random_forest_stratified(raisin_data, "Raisin")

evaluate_random_forest_stratified(titanic_data, "Titanic ")